# NAACL 2021 Tutorial: Beyond Paragraphs: NLP for Long Sequences


This NAACL 2021 tutorial will be held on Sunday, June 6, 2021.

## Tutorial Info
- Location: https://underline.io/events/122/sessions?eventSessionId=4103 (zoom link available)
- Time: 8am-12pm PST / 11am-3pm EST / 3pm-7pm GMT (Schedule TBA)

## Materials
- [Part 1. Intro & Overview of tasks](slides/Part 1 - Intro & Overview of tasks.pdf.pdf)
- [Part 2. Graph based methods](slides/Part 2 - Graph based methods.pdf)
- [Part 3. Long sequence transformers](Part 3 - Long sequence transformers.pdf)
- Part 4. Pretraining and finetuning
- Part 5. Use cases
- Part 6. Future work & conclusion

## Speakers
- Iz Beltagy (Al2)
- Arman Cohan (Al2)
- Hanna Hajishirzi (UW, Al2)
- Sewon Min (UW)
- Matthew Peters (AI2)

## Reading list

### Part 1. Intro & Overview of tasks
- Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher Potts. [Learning Word Vectors for Sentiment Analysis](https://www.aclweb.org/anthology/P11-1015/)
- Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David Corney, Benno Stein, Martin Potthast. [SemEval-2019 Task 4: Hyperpartisan News Detection](https://www.aclweb.org/anthology/S19-2145/)
- Yang et al. 2018. [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://arxiv.org/abs/1809.09600)
- Welbl et al. 2018. [Constructing Datasets for Multi-hop Reading Comprehension Across Documents](https://transacl.org/ojs/index.php/tacl/article/viewFile/1325/299)
- Napoles et al. 2012. [Annotated Gigaword](https://www.aclweb.org/anthology/W12-3018/)
- Cohan et al. 2018. [A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents](https://www.aclweb.org/anthology/N18-2097/)


### Part 2. Graph based methods
- Yang et al. 2016. [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
- Jain et al. 2020. [SciREX: A Challenge Dataset for Document-Level Information Extraction](https://www.aclweb.org/anthology/2020.acl-main.670/)
- Chang et al. 2019. [Language Model Pre-training for Hierarchical Document Representation](https://arxiv.org/abs/1901.09128)
- Zhang et al. 2019. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://www.aclweb.org/anthology/P19-1499)
- Lee et al. 2018. [Higher-order Coreference Resolution with Coarse-to-fine Inference](https://www.aclweb.org/anthology/N18-2108/)
- Wadden et al. 2019. [Entity, Relations, and Event Extraction with Contextualized Span Representations](https://www.aclweb.org/anthology/D19-1585/)
- Song et al. 2018. [Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks](https://arxiv.org/pdf/1809.02040.pdf)
- Xiao et al. 2019. [Dynamically Fused Graph Network for Multi-hop Reasoning](https://arxiv.org/pdf/1905.06933.pdf)
- Fang et al. 2020. [Hierarchical Graph Network for Multi-hop Question Answering](https://www.aclweb.org/anthology/2020.emnlp-main.710/)
- Min et al. 20219. [Knowledge-guided Text Retrieval and Reading for Open Domain Question Answering](https://arxiv.org/abs/1911.03868)

### Part 3. Long sequence transformers
- Dai et al. 2019. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- Rae et al. 2019. [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
- Roy et al. 2020. [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)
- Tay et al. 2020. [Sparse Sinkhorn Attention](https://arxiv.org/pdf/2002.11296.pdf)
- Kitaev et al. 2020. [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- Child et al. 2019. [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
- Beltagy et al. 2020. [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- Ainslie et al. 2020. [ETC: Encoding Long and Structured Inputs in Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.19/)
- Zaheer et al. 2020. [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
- Brown et al. 2020. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- Gray et al. 2017. [GPU Kernels for Block-Sparse Weights](https://cdn.openai.com/blocksparse/blocksparsepaper.pdf)
- Katharopoulos et al. 2020. [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
- Choromanski et al. 2020. [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)
- Peng et al. 2021. [Random Feature Attention](https://arxiv.org/abs/2103.02143)
- Wang et al. 2020. [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
- Tay et al. 2020. [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)

### Part 4. Pretraining and finetuning
- Xiong et al. 2021. [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902)
- Press et al. 2020. [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
- Caciularu et al. 2021. [Cross-Document Language Modeling](https://arxiv.org/abs/2101.00406)



















