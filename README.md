# NAACL 2021 Tutorial: Beyond Paragraphs: NLP for Long Sequences


This NAACL 2021 tutorial will be held on Sunday, June 6, 2021.

## Location & Time
- Location: [Underline.io link](https://underline.io/events/122/sessions?eventSessionId=4103) (zoom link available; accessible upon registration)
- Time: 8am-12pm PST / 11am-3pm EST / 3pm-7pm GMT (Schedule TBA)


## Speakers
- [Iz Beltagy](beltagy.net) (Al2) `beltagy@allenai.org`
- [Arman Cohan](armancohan.com) (Al2) `armanc@allenai.org`
- [Hanna Hajishirzi](homes.cs.washington.edu/~hannaneh/) (UW, Al2) `hannaneh@cs.washington.edu`
- [Sewon Min](shmsw25.github.io) (UW) `sewon@cs.washington.edu`
- [Matthew Peters](scholar.google.com/citations?user=K5nCPZwAAAAJ) (AI2) `matthewp@allenai.org`

## Materials
- [Part 1. Intro & Overview of tasks](slides/part1-intro-and-overview-of-tasks.pdf)
- [Part 2. Graph based methods](slides/part2-graph-based-methods.pdf)
- [Part 3. Long sequence transformers](slides/part3-long-sequence-transformers.pdf)
- [Part 4. Pretraining and finetuning](slides/part4-pretraining-and-finetuning.pdf)
- [Part 5. Use cases](slides/part5-use-cases.pdf)
- [Part 6. Future work & conclusion](slides/part6-future-work-and-conclusion)


## Reading list

### Part 1. Intro & Overview of tasks
- Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher Potts. [Learning Word Vectors for Sentiment Analysis](https://www.aclweb.org/anthology/P11-1015/)
- Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David Corney, Benno Stein, Martin Potthast. [SemEval-2019 Task 4: Hyperpartisan News Detection](https://www.aclweb.org/anthology/S19-2145/)
- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning. 2018. [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://arxiv.org/abs/1809.09600)
- Johannes Welbl, Pontus Stenetorp, Sebastian Riedel. 2018. [Constructing Datasets for Multi-hop Reading Comprehension Across Documents](https://transacl.org/ojs/index.php/tacl/article/viewFile/1325/299)
- Courtney Napoles, Matthew Gormley, Benjamin Van Durme. 2012. [Annotated Gigaword](https://www.aclweb.org/anthology/W12-3018/)
- Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, Nazli Goharian. 2018. [A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents](https://www.aclweb.org/anthology/N18-2097/)


### Part 2. Graph based methods
- Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. 2016. [Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174/)
- Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy. 2020. [SciREX: A Challenge Dataset for Document-Level Information Extraction](https://www.aclweb.org/anthology/2020.acl-main.670/)
- Ming-Wei Chang, Kristina Toutanova, Kenton Lee, Jacob Devlin. 2019. [Language Model Pre-training for Hierarchical Document Representation](https://arxiv.org/abs/1901.09128)
- Xingxing Zhang, Furu Wei, Ming Zhou. 2019. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://www.aclweb.org/anthology/P19-1499)
- Kenton Lee, Luheng He, Luke Zettlemoyer. 2018. [Higher-order Coreference Resolution with Coarse-to-fine Inference](https://www.aclweb.org/anthology/N18-2108/)
- David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi. 2019. [Entity, Relations, and Event Extraction with Contextualized Span Representations](https://www.aclweb.org/anthology/D19-1585/)
- Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, Daniel Gildea. 2018. [Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks](https://arxiv.org/abs/1809.02040)
- Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, Yong Yu. 2019. [Dynamically Fused Graph Network for Multi-hop Reasoning](https://arxiv.org/abs/1905.06933)
- Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, Jingjing Liu. 2020. [Hierarchical Graph Network for Multi-hop Question Answering](https://www.aclweb.org/anthology/2020.emnlp-main.710/)
- Sewon Min, Danqi Chen, Luke Zettlemoyer, Hannaneh Hajishirzi. 20219. [Knowledge-guided Text Retrieval and Reading for Open Domain Question Answering](https://arxiv.org/abs/1911.03868)

### Part 3. Long sequence transformers
- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. 2019. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap. 2019. [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
- Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier. 2020. [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)
- Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan. 2020. [Sparse Sinkhorn Attention](https://arxiv.org/abs/2002.11296)
- Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya. 2020. [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. 2019. [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
- Iz Beltagy, Matthew E. Peters, Arman Cohan. 2020. [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang. 2020. [ETC: Encoding Long and Structured Inputs in Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.19/)
- Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. 2020. [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
- Tom B. Brown et al. 2020. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- Scott Gray, Alec Radford and Diederik P. Kingma. 2017. [GPU Kernels for Block-Sparse Weights](https://cdn.openai.com/blocksparse/blocksparsepaper.pdf)
- Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret. 2020. [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
- Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller. 2020. [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)
- Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong. 2021. [Random Feature Attention](https://arxiv.org/abs/2103.02143)
- Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. 2020. [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
- Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler. 2020. [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)

### Part 4. Pretraining and finetuning
- Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh. 2021. [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902)
- Ofir Press, Noah A. Smith, Mike Lewis. 2020. [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
- Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie Cattan, Ido Dagan. 2021. [Cross-Document Language Modeling](https://arxiv.org/abs/2101.00406)



















